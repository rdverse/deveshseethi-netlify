---
title: Pruning
date: 2022-04-25
image: /assets/images/2021/coolimage.jpg
---

Currently working on this post!

Pruning is essential in resource-constrained environments such as smartwatches where bigger computations are taxing on the energy spent. Therefore, methods such as pruning \cite{castellano1997iterative} will help reduce the size of the over-parameterized neural network model by compressing the sparse weights in the graphs. This will result in model size reduction with a compromise for the model's precision. In our model, when we applied post-training pruning, the MAE increased to $0.28$ and hence we did not use pruning.
